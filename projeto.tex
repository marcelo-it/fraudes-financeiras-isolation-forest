\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage[T1]{fontenc}    
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{abnt-alf}
\usepackage[top=3cm,bottom=2cm,left=3cm,right=2cm]{geometry}
\usepackage{indentfirst}
\usepackage{float}
\usepackage{rotating}



\begin{document}

% CAPA
\pagestyle{empty}
% CAPA
\pagestyle{empty}
\begin{center}
\large  \textbf{UNIVERSIDADE PRESBITERIANA MACKENZIE} \\
\large  \textbf{PROGRAMA DE PÓS-GRADUAÇÃO EM}\\
\large  \textbf{COMPUTAÇÃO APLICADA}\\
\vskip 2.0cm
\textbf{\large João Marcelo Cattaldo Amorim}\\
\vskip 4.0cm
\setlength{\baselineskip}{1.5\baselineskip}
\textbf{\large IDENTIFICAÇÃO DE GOLPES FINANCEIROS EM INDÚSTRIAS DE BENS DE CONSUMO NA CONCESSÃO DE CRÉDITO A PESSOAS JURÍDICAS COM USO DE INTELIGÊNCIA ARTIFICIAL}\\
\vskip 4.5cm
\end{center}
\hfill{\vbox{\hsize=8.5cm\noindent\strut
Projeto de Pesquisa apresentado ao Programa de Pós-Graduação em Computação Aplicada da Universidade Presbiteriana Mackenzie como parte dos requisitos para obtenção do título de Mestre.}\\
\strut}
\vskip 3.0cm
\textbf{\normalsize Orientador: Prof. Dr. Leandro Augusto da Silva}\\
\vskip 2.0cm
\begin{center}
São Paulo\\
2024\\
\end{center}


% RESUMO
\newpage
\thispagestyle{plain}
\pagenumbering{roman}
\begin{center}
\large
\textbf{RESUMO}
\end{center}
\renewcommand{\baselinestretch}{0.6666666}
Esta dissertação investiga a identificação de padrões associados a golpes financeiros em indústrias de bens de consumo que concedem crédito a clientes pessoas jurídicas para a aquisição de seus produtos, utilizando técnicas de inteligência artificial e implementando soluções no fluxo operacional do motor de crédito.

A análise da base de dados revelou que aproximadamente 15\% dos clientes — cerca de 148.000 empresas — encontram-se em estado de inadimplência não recuperada, representando mais de R\$ 2,1 bilhões em dívidas. Esses clientes possuem um \textit{score} de crédito classificado como de alto risco.

O modelo de crédito atualmente utilizado pela empresa apresenta limitações significativas, desconsiderando fatores críticos que podem indicar golpes na concessão de crédito. Enquanto o \textit{score} de crédito tradicional baseia-se principalmente no histórico de pagamentos e nas restrições associadas ao cliente, golpes financeiros frequentemente envolvem alterações cadastrais recentes, como mudanças de endereço ou código de atividade econômica, além de comportamentos atípicos, como aumentos súbitos no volume de compras e consultas, e discrepâncias entre o tempo de atividade declarado e a data de fundação da empresa.

Para enfrentar os desafios de identificação de padrões relacionados a golpes financeiros e incorporar estas características no modelo de crédito, esta pesquisa desenvolveu uma solução baseada em modelos de \textit{machine learning}, abrangendo desde técnicas de classificação, por meio do algoritmo CBA (\textit{Classification Based Associations}), até técnicas avançadas de detecção de anomalias, como o \textit{Isolation Forest}. 

A técnica de detecção de anomalias demonstrou desempenho superior em comparação à classificação para este conjunto de dados, validando as principais hipóteses relacionadas aos padrões de fraudes. Essa abordagem mostrou-se eficiente na identificação de comportamentos atípicos e padrões indicativos de golpes financeiros, contribuindo de forma significativa para o aprimoramento da avaliação de risco de crédito e para a mitigação de perdas financeiras associadas a fraudes.
\\[0.5cm]
\begin{flushleft}
{\bf Palavras-chave:} {Análise de Crédito, \textit{Score} de crédito, Aprendizado de Máquina, \textit{Classification Based Associations}, \textit{Isolation Forest}, Detecção de Fraudes, Golpe Financeiro e CRISP-DM.}
\end{flushleft}

% SUMÁRIO
\newpage
\thispagestyle{empty}
\tableofcontents

% INTRODUÇÃO
\newpage
\pagestyle{plain}
\pagenumbering{arabic}
\renewcommand{\baselinestretch}{1.5}
\normalsize
\section{INTRODUÇÃO}
A CISP (Central de Informações São Paulo) é uma associação sem fins lucrativos fundada em 1972, com a missão de oferecer soluções exclusivas para análise de risco de crédito, contribuindo para o desenvolvimento econômico nacional. Composta por 192 grandes indústrias de produtos de largo consumo, organizadas em oito segmentos (Alimentos, Bebidas, Higiene Pessoal e Cosméticos, Papel, Papelaria, Utilidades Domésticas, Eletroeletrônicos e Produtos de Limpeza), a CISP representa aproximadamente 8\% do PIB brasileiro, destacando-se como uma associação de grande relevância no setor.

O sistema da CISP opera exclusivamente com clientes pessoas jurídicas, abrangendo cerca de 1.400.000 CNPJs. Os associados fornecem periodicamente informações comerciais de seus clientes, que são enriquecidas com dados de fontes públicas, como Receita Federal, Sintegra, Suframa e Protestos. Com base nessas informações, é gerada uma classificação de risco de performance (\textit{score} de crédito), que atribui notas de A (menor risco) a E (maior risco). Atualmente, 1.700 usuários das áreas de crédito e cobrança utilizam os relatórios da CISP, acessados manualmente, por API ou de forma automatizada via motor de crédito.

No entanto, fóruns realizados pela CISP com profissionais das áreas de crédito e cobrança identificaram limitações no modelo atual, que não considera fatores importantes relacionados a golpes financeiros. Exemplos incluem crescimento atípico de compras, alterações cadastrais recentes e discrepâncias entre a data de fundação e o tempo de atividade. Dados preliminares revelam que 15\% dos clientes — cerca de 148.000 empresas — encontram-se em inadimplência não recuperada, acumulando R\$ 2,1 bilhões em dívidas.

O objetivo deste estudo é propor e validar um modelo de inteligência artificial para identificar golpes financeiros, implementar a solução no fluxo atual do motor de crédito e mitigar os riscos enfrentados pelos associados da CISP. O modelo complementará o sistema atual, incorporando fatores específicos relacionados a golpes financeiros. Além de reduzir perdas financeiras, o estudo busca contribuir para o avanço das práticas de gestão de risco de crédito no setor e para a literatura acadêmica.

Para garantir a confidencialidade e a conformidade legal, o estudo utilizou dados anonimizados por meio do algoritmo de hash SHA-256, que transforma os CNPJs dos clientes em valores únicos e irreversíveis. A preparação dos dados e o desenvolvimento do modelo seguiram o framework CRISP-DM (\textit{Cross-Industry Standard Process for Data Mining}), uma metodologia amplamente reconhecida para projetos de ciência de dados.

Esta pesquisa utiliza algoritmos de inteligência artificial para detectar golpes financeiros, oferecendo uma solução técnica robusta que contribui tanto para a segurança financeira quanto para a inovação na gestão de risco de crédito.
Para garantir a confidencialidade e a conformidade legal, o estudo utilizou dados anonimizados por meio do algoritmo de hash SHA-256, que transforma os CNPJs dos clientes em valores únicos e irreversíveis. A preparação dos dados e o desenvolvimento do modelo seguiram o framework CRISP-DM (\textit{Cross-Industry Standard Process for Data Mining}), uma metodologia amplamente reconhecida para projetos de ciência de dados.

Esta pesquisa utiliza algoritmos de inteligência artificial para detectar golpes financeiros, oferecendo uma solução técnica robusta que contribui tanto para a segurança financeira quanto para a inovação na gestão de risco de crédito.
% REFERENCIAL TEÓRICO
\newpage
\section{REFERENCIAL TEÓRICO}
\label{sec:referencial}

O referencial teórico apresenta as bases conceituais e metodológicas que sustentam esta pesquisa, abordando os principais conceitos, teorias e estudos relacionados ao tema.

\subsection{Conceito de Crédito}
Crédito, derivado do latim \textit{credere} ou \textit{creditum} (confiança), significa acreditar, confiar e crer. Segundo \cite{rossato2020}, no contexto empresarial, representa a capacidade de pessoas e empresas adquirirem produtos ou serviços com pagamento futuro. \cite{Cardoso2024} complementam que o crédito também pode ser entendido como o montante disponibilizado ao cliente, seja em forma de empréstimo ou financiamento, mediante a promessa de pagamento em data futura.

\cite{ALEXANDRE2003} ressalta que, embora existam diversas definições para o termo crédito ou operação de crédito, é essencial compreender sua origem e sentido etimológico para uma melhor aplicação. Para Luiz Carlos Jacob Perera (apud \cite{ALEXANDRE2003}, "a história do crédito demonstra que sua evolução acompanhou o próprio desenvolvimento econômico da sociedade, procurando desenvolver instrumentos necessários para a satisfação das necessidades e anseios da humanidade. Além disso, o crédito, usado adequadamente, tanto por governos quanto por empresas, como forma de gestão do consumo, continua a mostrar vigor notável, graças ao papel importante que desempenha no cotidiano da humanidade como instrumento provocador e facilitador das transações de bens e serviços."
\subsection{Concessão de Crédito}

A concessão de crédito é sempre uma decisão incerta, porém tem sido um componente importante no desenvolvimento e crescimento da economia e do país. Ela se dá no momento em que a instituição se sente segura ao ponto de entregar a sua mercadoria ou capital, afirma \cite{beserra2022}. 

Para \cite{rossato2020}, é um instrumento para alavancar vendas, disponibilizado por agências financiadoras, cooperativas de crédito e empreendimentos que permitem aos clientes adquirir bens e serviços com pagamento futuro. Além disso, o crédito é destacado como um fator essencial para o desenvolvimento econômico, ao viabilizar a produção de bens e serviços pelos empresários. 

“Fatores como o aumento do grau de estabilidade econômica, surgimento de novos produtos e serviços e controle da inflação contribuem para a ampliação do mercado consumidor”, diz Rodrigo Ventura (apud \cite{fuhr2022}. \cite{fuhr2022} ressaltam que o crescimento de pessoas e empresas no mercado nacional impulsiona e reposiciona a importância da análise de crédito, consolidando seu valor na gestão empresarial e na economia. 

Isso ocorre porque as empresas frequentemente optam por comercializar seus produtos e serviços a prazo, exigindo critérios claros para avaliar e decidir sobre a concessão de crédito. Esse processo envolve a análise do risco de inadimplência, ou seja, a possibilidade de não pagamento dos valores acordados. Segundo Gouvêa Gonçalves (apud \cite{fuhr2022}), “A avaliação do risco tem por objetivo melhorar a qualidade de uma carteira de clientes, favorecendo uma venda saudável e evitando ao máximo a perda de valores, sobre créditos fornecidos de forma equivocada ou a clientes que geram prejuízos aos negócios. Empresas que possuem boa avaliação levam vantagens sobre seus concorrentes”. 

\cite{montevechi2022} citam que a concessão de crédito, fundamental para a indústria financeira, envolve uma relação contratual entre credores e tomadores, assumindo riscos inerentes, como a inadimplência. Para mitigar esses riscos, instituições financeiras desenvolvem metodologias para avaliar a probabilidade de não pagamento e classificar os tomadores com base em dados históricos. Entre essas ferramentas, destaca-se o \textit{score} de crédito, que utiliza modelos matemáticos para prever o comportamento dos devedores e estimar a \textit{probability of default}, contribuindo para a sustentabilidade do sistema creditício e o sucesso das operações financeiras.
\subsection{Avaliação de Risco e \textit{Score} de crédito}

Para Gouvêa Gonçalves (apud \cite{fuhr2022}), a avaliação de risco visa melhorar a qualidade da carteira de clientes, evitando perdas por créditos mal concedidos e promovendo vendas saudáveis, conferindo vantagem competitiva às empresas que realizam boas análises. A tecnologia de \textit{score} de crédito contribuiu significativamente para a diminuição dos custos relacionados à análise de crédito, garantindo consistência nas informações, maior velocidade e acurácia na tomada de decisão.

Os modelos de \textit{score} de crédito são baseados em informações fornecidas pelos próprios solicitantes e, por meio de técnicas estatísticas e matemáticas, atribuem pontuações que distinguem bons e maus pagadores, segundo \cite{beserra2022}. De acordo \cite{francisco2012}, a classificação do risco de crédito é vista como uma ferramenta fundamental para analistas e instituições.

\cite{fuhr2022} destaca que a classificação de crédito tem como objetivo desenvolver um modelo que integre informações quantitativas e qualitativas sobre a credibilidade da empresa, refletindo a qualidade do devedor. O \textit{score} de crédito, por sua vez, tem como principal propósito avaliar o risco de inadimplência com base em uma pontuação, indicando a probabilidade de um candidato não honrar com seus compromissos acordados.

\subsection{Motores de Decisão de Crédito}

De acordo com a \cite{serasa2024}, os motores de decisão de crédito são ferramentas que auxiliam na tomada de decisões financeiras, otimizando processos e reduzindo riscos operacionais. Esses sistemas utilizam análises avançadas e combinações de dados internos e de mercado para fornecer decisões precisas, baseadas no perfil de risco de cada cliente.

\subsubsection{Principais Benefícios dos Motores de Decisão}

\begin{enumerate}
    \item \textbf{Agilidade e Eficiência:} Automatizam a análise de crédito, reduzindo o tempo de resposta e otimizando processos operacionais.
    \item \textbf{Redução de Riscos:} Diminuem a exposição a fraudes e inadimplências, garantindo decisões mais seguras com ferramentas específicas, como validação cadastral e \textit{scores} de fraude.
    \item \textbf{Melhoria na Identificação de Clientes:} Identificam clientes com baixo risco, tornando as negociações mais vantajosas e seguras.
    \item \textbf{Automação e Padronização:} Simplificam estruturas operacionais e aumentam a consistência nas análises de crédito, alinhadas às políticas da empresa.
    \item \textbf{Aumento de Vendas:} Aceleram o processo de decisão, permitindo que as vendas sejam concluídas mais rapidamente.
\end{enumerate}

A operação do motor de decisão baseia-se em regras de negócio predefinidas e na análise dos dados inseridos no sistema. A ferramenta calcula o perfil de risco do cliente por meio de \textit{scores} e indicadores. 

De acordo com a \cite{deps_sd}, os motores de decisão de crédito são soluções estratégicas para empresas de diferentes portes e setores, proporcionando agilidade, segurança e melhores resultados financeiros, enquanto minimizam riscos e otimizam processos.

\subsubsection{Como Funciona}

O motor de decisão de crédito é uma tecnologia que automatiza o processo de análise e concessão de crédito, combinando algoritmos de análise e bases de dados robustas. Ele avalia o risco de inadimplência dos clientes, define limites seguros e agiliza a aprovação de vendas a prazo. Em sistemas adaptados, permite que vendedores realizem análises diretamente, aumentando a produtividade e poupando tempo.

\subsubsection{Vantagens dos Motores de Decisão}

\begin{enumerate}
    \item \textbf{Maior Fluxo de Informações:} Atualiza continuamente a base de dados com novos históricos de pagamentos, aprendendo e ajustando parâmetros para maior precisão.
    \item \textbf{Estrutura Simplificada:} Reduz a necessidade de equipes para análise manual, permitindo que profissionais se concentrem em funções estratégicas e casos excepcionais.
    \item \textbf{Segurança para Ampliar Limites:} Identifica perfis de clientes de baixo risco, possibilitando a concessão de limites maiores, mesmo para novos clientes.
    \item \textbf{Melhor Experiência de Compra:} A análise rápida garante respostas quase imediatas sobre a concessão de crédito, acelerando o processo de vendas e otimizando a experiência do cliente.
\end{enumerate}

Os motores de decisão de crédito representam uma solução eficiente e estratégica para empresas que buscam modernizar seus processos de crédito, reduzindo custos, aumentando a precisão e melhorando a experiência do cliente.

\subsection{Golpes Financeiros e Detecção de Fraudes}
A fraude financeira representa uma ameaça crescente, com impactos negativos significativos no setor financeiro e na sociedade. \cite{martins2022} destacam que, embora a mineração de dados seja eficaz na detecção de fraudes, ela enfrenta desafios devido à constante mudança nos perfis comportamentais e à similaridade entre transações fraudulentas e legítimas. Os autores também salientam que a prevenção de fraudes é uma abordagem proativa, focada em evitar sua ocorrência, enquanto a detecção atua de forma reativa, identificando transações fraudulentas em andamento.

Para Santos (apud \cite{soares2024}), "fraude é um processo sistemático de ações cujo objetivo é distorcer dados intencionalmente e que se estabelece quando agentes internos e externos da companhia possuem a intenção de agir dissimuladamente." Conforme \cite{soares2024}, a Associação dos Investigadores de Fraude Certificados (ACFE) classifica as fraudes em três categorias principais:
\begin{enumerate}
    \item \textbf{Corrupção}: Envolve subornos ou uso indevido de bens públicos.
    \item \textbf{Apropriação indevida de ativos}: Inclui fraudes que impactam ou não diretamente o caixa da empresa.
    \item \textbf{Demonstrações financeiras fraudulentas}: Compreendem receitas fictícias e ocultação de passivos e despesas.
\end{enumerate}

Entre as fraudes mais comuns no Brasil, as relacionadas a contas a pagar e contas a receber representam 23\% do total de investigações de fraudes conduzidas, sendo detectadas, em sua maioria, por denúncias anônimas, denúncias nominais e atividades de auditoria interna.

Segundo \cite{thinkdata2024}, a prevenção de fraudes na concessão de crédito é um tema de crescente importância no contexto financeiro brasileiro, especialmente diante do aumento expressivo de tentativas fraudulentas, como roubo de identidade e apresentação de informações falsas. O cenário atual exige que instituições financeiras e empresas adotem estratégias robustas e proativas para identificar e mitigar esses riscos, protegendo a integridade do sistema financeiro e os consumidores. Dados recentes mostram que, em 2022, o Brasil registrou quase 3,9 milhões de tentativas de fraude de identidade, evidenciando a vulnerabilidade do setor financeiro.

Entre os principais desafios da prevenção de fraudes na concessão de crédito estão:
\begin{itemize}
    \item \textbf{Risco de inadimplência}: Requer uma análise precisa da capacidade do cliente de cumprir suas obrigações financeiras, utilizando históricos de crédito e modelos de risco.
    \item \textbf{Avanço tecnológico}: Acompanhamento da evolução das tecnologias para garantir segurança e inovação nos processos de análise.
    \item \textbf{Fraudes sofisticadas}: Enfrentar técnicas cada vez mais avançadas de roubo de identidade e falsificação documental, demandando medidas rigorosas de validação.
\end{itemize}

Conforme mencionado por \cite{maniraj2019}, a fraude, definida como um ato ilícito ou criminal para obtenção de benefícios financeiros ou pessoais, é um desafio significativo no setor financeiro. Diversos estudos exploram técnicas para detecção de fraudes, como mineração de dados, aprendizado supervisionado e não supervisionado, e detecção adversarial. Embora esses métodos tenham alcançado sucesso em algumas áreas, ainda enfrentam limitações na criação de soluções permanentes e consistentes.
\subsection{Detecção de Anomalias}

Conforme mencionado por \cite{gupta2020}, a detecção de anomalias é um método para identificar ocorrências suspeitas de eventos e itens de dados que podem causar problemas para as autoridades competentes. As anomalias nos dados geralmente estão associadas a questões como problemas de segurança, falhas em servidores, fraudes bancárias, falhas estruturais em edifícios, defeitos clínicos, entre outros.

\subsection{Aprendizado de Máquina e Detecção de Fraudes}

De acordo com Maxwell (apud \cite{martins2022}, o aprendizado de máquina (ML – \textit{Machine Learning}) é o estudo de algoritmos de computador que se aprimoram automaticamente com a experiência. É tratado como uma subárea da inteligência artificial (IA). Algoritmos de aprendizado de máquina constroem um modelo baseado em dados de amostra, conhecidos como “dados de treinamento”, a fim de fazer previsões ou decisões sem serem explicitamente programados para isso. Esses algoritmos são usados em uma ampla variedade de aplicações, como filtragem de e-mails e visão computacional, em que é difícil ou inviável desenvolver algoritmos convencionais para realizar as tarefas necessárias.

\cite{martins2022} citam que as principais instituições financeiras, tanto nacionais quanto internacionais, têm adotado algoritmos de aprendizado de máquina para analisar dados, alcançando resultados financeiros significativos. Essa tecnologia tem sido aplicada com sucesso em áreas como análise de risco de crédito, previsão de falências, estimativas de cotações de moedas e ações, segmentação de mercado e detecção de fraudes.

Para Xuan (apud \cite{martins2022}), a detecção de fraudes busca identificar transações atípicas, que podem ocorrer em diferentes contextos, como operações financeiras, consumo de energia, compras, uso de recursos sociais, acesso a redes de computadores e manipulação contábil. Algoritmos de aprendizado de máquina são amplamente utilizados para classificar transações como legítimas ou fraudulentas, configurando um problema de classificação binária. No entanto, a análise enfrenta desafios, como a predominância de transações legítimas nos dados e a constante criação de novas fraudes, exigindo a adaptação contínua dos modelos. Por isso, a detecção de fraudes também é vista como um problema de fluxo contínuo de dados.

\cite{gupta2020} mencionam que o aprendizado de máquina oferece métodos eficazes para extrair informações úteis de grandes volumes de dados, auxiliando na tomada de decisão e aumentando a precisão preditiva. No contexto da detecção de fraudes em geral, o foco principal é diferenciar transações fraudulentas das legítimas. O treinamento de sistemas de detecção de fraudes pode ser realizado de três formas:
\begin{enumerate}
    \item \textbf{Supervisionado:} Utiliza conjuntos de dados rotulados, nos quais os itens possuem informações detalhadas e estão previamente classificados. O modelo é treinado para analisar novos dados e realizar a classificação.
    \item \textbf{Semissupervisionado:} Combina dados rotulados e não rotulados, sendo mais utilizado que o método supervisionado. Essa abordagem é ideal para cenários em que há maior disponibilidade de dados não rotulados.
    \item \textbf{Não supervisionado:} Baseia-se em dados não rotulados e identifica padrões anômalos de forma autônoma, assumindo que exceções são raras no conjunto de dados. É a estratégia mais utilizada, especialmente para tarefas mais complexas, embora possa ser mais imprevisível.
\end{enumerate}

Para \cite{bhati2024}, o aprendizado de máquina (ML), um ramo da inteligência artificial (IA), apresenta-se como uma abordagem promissora para a detecção de fraudes, permitindo que sistemas aprendam com dados históricos e melhorem suas previsões sem a necessidade de programação manual. No entanto, sua aplicação enfrenta desafios consideráveis, como a anonimização dos dados transacionais, que dificulta a reprodução de estudos, e a natureza dinâmica e desbalanceada das fraudes, que compromete a precisão dos modelos atuais. Esses obstáculos reforçam a necessidade de desenvolver soluções mais eficazes e robustas.

\subsection{Metodologia CRISP-DM}

A análise de grandes volumes de dados tornou-se um pilar central nas estratégias de TI corporativas, desempenhando um papel significativo na tomada de decisões estratégicas \cite{schroer2021}. A ciência de dados, por meio de modelos matemáticos e analíticos, destaca-se ao adotar metodologias estruturadas amplamente reconhecidas como fatores críticos para o sucesso. Entre essas, a metodologia CRISP-DM (\textit{Cross Industry Standard Process for Data Mining}) é amplamente aplicada devido à sua flexibilidade e aplicabilidade em diferentes setores \cite{silva2023}.

O CRISP-DM é um modelo consolidado, composto por seis fases iterativas que abrangem desde o entendimento do negócio até a implantação. Sua estrutura cíclica permite ajustes contínuos com base nos aprendizados de processos anteriores, garantindo flexibilidade e evolução constante \cite{brzozowska2023}. A sequência das fases não é rígida, permitindo revisões e adaptações conforme os resultados evoluem.

\subsubsection{Fases do CRISP-DM}

\begin{enumerate}
    \item \textbf{Entendimento do Negócio:} Define os objetivos do projeto sob uma perspectiva empresarial, traduzindo-os em problemas de mineração de dados e criando um plano inicial.
    \item \textbf{Compreensão dos Dados:} Envolve a coleta inicial e análise exploratória, identificando problemas de qualidade e padrões relevantes para hipóteses iniciais.
    \item \textbf{Preparação dos Dados:} Inclui a construção do conjunto de dados final, envolvendo seleção, limpeza, transformação e criação de atributos derivados.
    \item \textbf{Modelagem:} Técnicas de modelagem são aplicadas e ajustadas, garantindo alinhamento com os critérios definidos e otimização dos parâmetros.
    \item \textbf{Avaliação:} Os modelos são avaliados em relação aos objetivos de negócios, revisando todo o processo para identificar possíveis lacunas.
    \item \textbf{Implantação:} Organiza e apresenta o conhecimento gerado de forma útil ao cliente, variando desde relatórios simples até integração em processos decisórios.
\end{enumerate}

Pesquisas destacam a ampla adoção da mineração de dados em setores como o financeiro, onde metodologias como KDD, SEMMA e CRISP-DM têm sido utilizadas para assegurar benefícios consistentes \cite{plotnikova2022}.

\cite{brzozowska2023} complementam essa análise, demonstrando que o CRISP-DM lidera sua aplicação em 42\% dos casos de mineração de dados, seguido por metodologias proprietárias (19\%) e pela SEMMA (13\%). A pesquisa também enfatiza a importância da preparação e compreensão dos dados como etapas mais trabalhosas do processo, enquanto as fases subsequentes seguem o modelo sistemático descrito. Essa abordagem cíclica garante que o modelo evolua continuamente, respondendo a novas questões de negócios com base nos aprendizados obtidos.

\subsection{CBA (\textit{Classification Based Associations})}

O CBA \textit{(Classification Based Associations)} é um algoritmo de aprendizado supervisionado amplamente utilizado devido à sua simplicidade, eficácia e capacidade de gerar classificadores precisos e interpretáveis. Desde sua criação, ele tem sido aplicado em uma variedade de tarefas de classificação, sendo especialmente reconhecido no campo de Regras de Associação Classificadoras ARC \cite{westermann2019}.

O algoritmo utiliza Regras de Associação Classificadoras (CARs) como base, organizando e selecionando um subconjunto relevante para a classificação de características. Sua abordagem combina interpretabilidade, precisão e rapidez na construção do modelo, sendo dividido em duas fases principais:
\begin{enumerate}
    \item \textbf{Gerador de Regras (CBA-RG):} Emprega o algoritmo \textit{Apriori} para identificar CARs que atendam aos critérios mínimos de suporte e confiança, utilizando dados históricos para minerar regras frequentes \cite{kumi2021}.
    \item \textbf{Construtor do Classificador (CBA-CB):} Refina as regras geradas por meio de ordenação e poda, priorizando aquelas com maior suporte e confiança, garantindo precisão e interpretabilidade no classificador final \cite{filip2018}.
\end{enumerate}

Duas versões do algoritmo foram propostas: \textit{M1}, que realiza múltiplas varreduras no banco de dados para otimizar a seleção de regras, e \textit{M2}, projetada para cenários com grandes volumes de dados \cite{filip2018}. No entanto, avanços tecnológicos recentes favorecem o desempenho do \textit{M1}, como demonstrado em benchmarks recentes, que destacaram sua superioridade em eficiência na maioria dos cenários avaliados.

O desenvolvimento do modelo \textit{CBA} segue três etapas principais que envolvem a preparação dos dados, a geração de regras e a construção do classificador. Essas etapas são detalhadas a seguir:
\begin{enumerate}
    \item \textbf{Discretização de variáveis contínuas:} Dados contínuos são transformados em dados discretizados utilizando métodos como divisão por intervalos iguais, divisão por frequência igual, discretização baseada em entropia ou em estimativa de densidade de kernel \cite{liu2020}. Nesta etapa, o número adequado de intervalos deve ser definido para garantir entradas confiáveis.
    \item \textbf{Geração de CARs:} As regras são criadas pelo algoritmo \textit{Apriori}, atendendo aos limites mínimos de suporte (\textit{minsup}) e confiança (\textit{minconf}) definidos pelo usuário.
    \item \textbf{Poda e refinamento das CARs:} As regras são organizadas de acordo com critérios de precedência, que priorizam aquelas com maior confiança. Em caso de empates, regras com maior suporte são escolhidas, e, se necessário, utiliza-se a ordem de geração como critério final. Esse processo garante que as regras mais relevantes sejam selecionadas para compor o classificador final. As \textit{CARs} refinadas (\textit{pCARs}) são filtradas para formar o conjunto final de regras do classificador.
\end{enumerate}

O classificador final seleciona um conjunto de regras ordenadas para cobrir os dados de treinamento. Para novos casos, a primeira regra compatível com o caso é utilizada para classificá-lo; se nenhuma regra for aplicável, o caso é atribuído à classe padrão.

Além de prever alvos em novos dados, o \textit{CBA} gera regras legíveis e compreensíveis, facilitando sua aplicação em contextos práticos e acadêmicos. Essas \textit{CARs} também têm potencial para pós-mineração de regras e descoberta de conhecimento, ampliando sua utilidade e contribuindo para insights adicionais sobre os dados analisados.

\subsection{\textit{Isolation Forest}}

O Isolation Forest é definido como um algoritmo de detecção de anomalias eficiente e independente de modelos, capaz de identificar amostras anômalas sem a necessidade de construir perfis detalhados dos dados \cite{chen2023}. Desenvolvido por Liu et al. (2008), o algoritmo utiliza árvores para isolar instâncias anômalas, baseando-se no comprimento médio do caminho entre os nós analisados e o nó raiz. Essa abordagem elimina a necessidade de cálculos de distância e densidade, aproveitando as características das anomalias — pontos raros e distintos — para separá-las rapidamente dos dados regulares, com eficiência temporal linear e baixo consumo de memória.

Apesar de sua eficácia, o algoritmo apresenta limitações importantes:
\begin{itemize}
    \item \textbf{Aleatoriedade elevada:} A geração de árvores com amostragem aleatória pode reduzir a capacidade de detecção caso os subconjuntos não contenham anomalias.
    \item \textbf{Desempenho de generalização limitado:} O aumento no número de árvores frequentemente gera estruturas semelhantes, reduzindo a eficiência e aumentando os custos computacionais.
    \item \textbf{Estabilidade reduzida:} A necessidade de configurar hiperparâmetros manualmente pode levar a resultados inconsistentes entre diferentes conjuntos de dados.
\end{itemize}

Essas limitações reforçam a necessidade de melhorias e adaptações para aumentar a robustez e a eficiência do algoritmo em diferentes contextos. A exploração de propriedades dos dados anômalos, como baixa frequência e diferenças marcantes em relação aos dados normais, continua sendo a base para seu desempenho, embora ajustes sejam essenciais para lidar com questões como aleatoriedade e estabilidade.

\cite{gupta2020}. descrevem o Isolation Forest como um algoritmo não supervisionado que utiliza um conjunto de árvores de decisão para identificar anomalias. Em vez de calcular distâncias entre pontos ou criar perfis de instâncias normais, ele isola anomalias particionando os dados. Os pontos com o menor comprimento médio de caminho nas árvores de decisão são classificados como anômalos.

\subsubsection{Métricas de Avaliação}
Como o Isolation Forest é uma técnica não supervisionada, são necessárias métricas que independam de limiares de predição e ofereçam uma pontuação precisa. Entre as mais adequadas estão:
\begin{itemize}
    \item \textbf{AUC (Área sob a Curva ROC):} Mede a capacidade de um modelo binário de distinguir entre verdadeiros e falsos positivos, com valores variando de 0,5 (base) a 1 (ideal).
    \item \textbf{AUCPR (Área sob a Curva de Precisão-Revocação):} Analisa a relação entre precisão e revocação em diferentes limiares. É especialmente útil para conjuntos de dados desbalanceados devido à sua sensibilidade a verdadeiros positivos, falsos negativos e falsos positivos, ignorando os verdadeiros negativos.
\end{itemize}

\subsubsection{Etapas de Implementação}
A implementação do Isolation Forest envolve três etapas principais: seleção de hiperparâmetros, treinamento do modelo e avaliação com métricas apropriadas.

\subsubsection{Seleção de Hiperparâmetros}
Os dois hiperparâmetros principais do Isolation Forest são:
\begin{itemize}
    \item \textbf{Número de Árvores:} Define a quantidade de árvores na floresta. Mais árvores podem aumentar a precisão na detecção de anomalias, mas também elevam a complexidade computacional.
    \item \textbf{Nível de Contaminação:} Representa a proporção estimada de anomalias no conjunto de dados. Esse parâmetro deve ser ajustado com validação cruzada ou conhecimento especializado para equilibrar precisão e revocação.
\end{itemize}
\subsubsection{Treinamento do Modelo}
Após definir os hiperparâmetros, o modelo é treinado utilizando dados pré-processados \cite{meduri2024}. Durante o treinamento, as árvores de isolamento dividem o espaço de características aleatoriamente até isolar cada ponto de dado. Pontuações de anomalia são atribuídas com base no comprimento médio do caminho necessário para isolar cada ponto, sendo os valores mais altos associados a possíveis anomalias.

\subsubsection{Seleção e Impacto do Limiar}
O limiar de pontuação de anomalia diferencia dados normais de anômalos, afetando o equilíbrio entre precisão (capacidade de evitar falsos positivos) e revocação (capacidade de detectar a maioria das anomalias). Métodos avançados, como curvas ROC-AUC e métricas específicas, ajudam a selecionar limiares adequados considerando o desbalanceamento das classes e a eficácia do modelo.

\subsubsection{Avaliação do Modelo com métricas apropriadas}
A avaliação do modelo considera quatro métricas principais:
\begin{enumerate}
    \item \textbf{Precisão:} Mede a proporção de anomalias corretamente identificadas entre todas as classificadas como anômalas.
    \item \textbf{Revocação:} Avalia a capacidade do modelo de identificar a maioria das anomalias reais.
    \item \textbf{F1-Score:} Combina precisão e revocação para cenários de classes desbalanceadas, utilizando a média harmônica.
    \item \textbf{ROC-AUC:} Reflete o desempenho geral do modelo ao medir a relação entre a taxa de verdadeiros positivos e falsos positivos em diferentes limiares.
\end{enumerate}

Essas etapas garantem que o modelo seja otimizado para identificar anomalias de maneira eficaz, mesmo em cenários com dados desbalanceados e complexos, como na detecção de fraudes.



\section{DESENVOLVIMENTO}
\subsection{Metodologia}

A presente pesquisa adota uma abordagem quantitativa, descritiva e explicativa, fundamentada em procedimentos experimentais e documentais. Por meio de um raciocínio dedutivo, a investigação empírica explora a aplicação do algoritmo \textit{Isolation Forest} em dados históricos de transações financeiras para a identificação de fraudes hipoteticamente conhecidas. 

A eficácia do modelo é avaliada utilizando métricas específicas de desempenho, como:
\begin{itemize}
    \item \textbf{Precisão:} Mede a proporção de verdadeiros positivos entre todas as classificações positivas realizadas pelo modelo.
    \item \textbf{Recall:} Avalia a capacidade do modelo de identificar a maior quantidade possível de fraudes reais.
    \item \textbf{AUCPR (\textit{Área Sob a Curva de Precisão-Revocação}):} Analisa o equilíbrio entre precisão e revocação, sendo especialmente adequada para conjuntos de dados desbalanceados.
\end{itemize}

A metodologia empregada segue o framework CRISP-DM (\textit{Cross Industry Standard Process for Data Mining}), que orienta a organização do processo em etapas claras e iterativas, garantindo flexibilidade e adaptação ao longo do estudo. As etapas incluem:
\begin{enumerate}
    \item \textbf{Entendimento do Negócio:} Definir os objetivos e os requisitos da pesquisa.
    \item \textbf{Compreensão dos Dados:} Realizar a coleta e análise exploratória dos dados históricos das transações financeiras.
    \item \textbf{Preparação dos Dados:} Selecionar, limpar e transformar os dados para adequação ao modelo.
    \item \textbf{Modelagem:} Aplicar o algoritmo \textit{Isolation Forest} com ajustes nos hiperparâmetros.
    \item \textbf{Avaliação:} Validar os resultados utilizando as métricas de desempenho mencionadas.
    \item \textbf{Implantação:} Gerar relatórios e recomendações para integração do modelo no fluxo de análise de crédito.
\end{enumerate}

\subsection{Entendimento do Negócio}

A CISP - Central de Informações São Paulo é uma associação sem fins lucrativos, fundada em 1972, com a missão de tornar-se referência no desenvolvimento de ferramentas técnicas e tecnológicas para a gestão de risco de crédito. Atualmente, a associação congrega 192 grandes indústrias de bens de consumo, organizadas em oito segmentos de atuação: Alimentos, Bebidas, Higiene Pessoal e Cosméticos, Papel, Papelaria, Utilidades Domésticas e Eletroeletrônicos.

A filiação à CISP está sujeita a critérios rigorosos, definidos pela assembleia geral composta pelos representantes das indústrias associadas. Esses critérios incluem o volume mínimo de clientes e o faturamento anual. Diferentemente de um \textit{bureau} de crédito tradicional, a CISP caracteriza-se como um \textit{hub} de troca de informações, promovendo a colaboração entre seus associados por meio do compartilhamento de dados.

Regida por um estatuto, a associação estabelece diretrizes claras e obrigações para seus membros, sendo a reciprocidade um dos principais requisitos. A reciprocidade exige que os associados enviem, periodicamente, arquivos contendo informações comerciais de seus clientes. Esses dados são submetidos a processos de validação e enriquecimento, garantindo sua consistência e qualidade. A base de dados da CISP é composta exclusivamente por clientes ativos — definidos como aqueles que realizaram transações comerciais nos últimos 12 meses ou, em casos excepcionais, possuem débitos pendentes fora desse período.

\subsubsection{Enriquecimento de Dados}

O enriquecimento da base de dados inclui a integração de informações cadastrais provenientes de fontes públicas, como:
\begin{itemize}
    \item Receita Federal do Brasil;
    \item QSA (Quadro de Sócios e Administradores);
    \item Sintegra;
    \item Suframa;
    \item Regime Tributário do Simples Nacional.
\end{itemize}

Além disso, são incorporadas informações restritivas, como:
\begin{itemize}
    \item Registros de cheques sem fundos, obtidos diretamente do CCF BACEN;
    \item Dados de protestos, coletados no CENPROT;
    \item Informações relacionadas a recuperação judicial, dívidas vencidas, cobranças amigáveis e registros de possíveis fraudes, fornecidas tanto pelos associados quanto pela própria CISP.
\end{itemize}

Com base nos dados enriquecidos, a CISP aplica um modelo de \textit{score} de crédito, que avalia o risco de performance dos clientes e atribui notas de A (excelente) a E (deficiente). Essa classificação oferece aos associados uma base objetiva para a tomada de decisões sobre concessão de crédito. Entretanto, o modelo atual não considera fatores adicionais relacionados ao risco de crédito, como alterações cadastrais frequentes ou aumentos atípicos no volume de compras. Essas informações são consolidadas em relatórios detalhados disponibilizados pela CISP e analisadas pelos tomadores de decisão, exclusivamente para a concessão de crédito a pessoas jurídicas.

\subsubsection{Identificação de Padrões Comportamentais}

Adicionalmente, a CISP promove a interação entre os profissionais de crédito e cobrança de seus associados por meio de plataformas e encontros periódicos. Durante esses fóruns, foram identificados padrões comportamentais associados a clientes fraudulentos. Em muitos casos, as fraudes envolveram empresas que inicialmente apresentavam boas classificações de risco de crédito e realizavam compras regulares, mas que, em um curto período, exibiram comportamentos de inadimplência significativa, configurando potenciais golpes financeiros.

Essas discussões permitiram levantar hipóteses sobre padrões comportamentais que não são adequadamente capturados pelos modelos tradicionais de análise de risco. Entre esses padrões destacam-se:
\begin{enumerate}
    \item \textbf{Alterações Cadastrais:} Mudanças em dados cadastrais, como endereço, razão social, CNAE ou e-mail.
    \item \textbf{Crescimento Atípico no Volume de Compras:} Aumentos repentinos e incomuns no volume de compras realizadas por um cliente.
    \item \textbf{Comportamento no Volume de Consultas:} Elevação anormal na quantidade de consultas realizadas sobre o CNPJ de um cliente em um curto período.
    \item \textbf{Tempo de Atividade vs. Data de Fundação:} Inconsistências entre a data de fundação da empresa e seu período recente de atividade.
    \item \textbf{Crescimento Atípico no Débito Total:} Incrementos súbitos no volume total de débitos acumulados pelo cliente.
    \item \textbf{Crescimento na Quantidade de Associadas no Débito Total:} Aumento expressivo no número de empresas associadas ao débito total do cliente.
    \item \textbf{Crescimento Atípico no Valor do Maior Acúmulo:} Expansões repentinas no maior valor acumulado registrado para o cliente.
    \item \textbf{Crescimento na Quantidade de Associadas no Maior Acúmulo:} Elevação significativa no número de empresas relacionadas ao maior acúmulo de débitos do cliente.
\end{enumerate}

\subsubsection{Risco de Crédito e Impactos Financeiros}

Na base de dados da CISP, que totaliza aproximadamente 1,4 milhão de CNPJs ativos do mercado interno, estima-se que cerca de 15\% dos registros apresentem inadimplência severa, classificados como risco D ou E. Essas condições indicam dificuldades extremas, ou mesmo a impossibilidade, de recuperação dos valores devidos. Atualmente, não é possível mensurar com precisão quais desses casos configuram, de fato, golpes financeiros. Juntas, as dívidas desses CNPJs totalizam mais de R\$ 2,1 bilhões, representando um risco significativo para os associados.

Nesta etapa, foi possível mapear e categorizar algumas hipóteses de golpes financeiros que afetam as indústrias associadas, além de compreender os potenciais benefícios do desenvolvimento de uma solução para identificá-los. Esse entendimento reforçou a importância de mitigar fraudes financeiras como meio de reduzir perdas, proteger os ativos das empresas e aprimorar a eficiência e segurança nos processos de concessão de crédito.


\subsection{Materiais e Ferramentas}

O estudo foi realizado utilizando a versão gratuita da plataforma \textbf{Google Colab}, uma ferramenta baseada em nuvem que permite a execução de experimentos computacionais de forma interativa e escalável. O Google Colab oferece recursos pré-configurados e acessíveis para projetos de ciência de dados e aprendizado de máquina, eliminando a necessidade de infraestrutura local.

\subsubsection{Configuração da Plataforma}
O ambiente de execução foi configurado no back-end gratuito do Google Compute Engine, utilizando o interpretador Python 3. As especificações técnicas disponíveis incluíram 12,7 GB de memória RAM e 225,8 GB de disco temporário. Apesar de limitações em termos de persistência dos dados e acesso a GPUs ou TPUs em sessões prolongadas, a versão gratuita do Colab foi suficiente para conduzir todas as etapas do estudo, incluindo a manipulação de um \textit{dataset} com 1.777.725 linhas e 19 colunas.

\subsubsection{Linguagem de Programação e Bibliotecas Utilizadas}
O estudo foi desenvolvido inteiramente em \textbf{Python}, versão 3, devido à sua versatilidade e ampla utilização em projetos de ciência de dados e aprendizado de máquina. As principais bibliotecas utilizadas foram:

\begin{itemize}
    \item \textbf{Pandas:} Para manipulação e análise de dados, garantindo organização, limpeza e transformação do \textit{dataset}.
    \item \textbf{NumPy:} Para operações numéricas eficientes e manipulação de arrays multidimensionais.
    \item \textbf{Hashlib:} Para anonimização dos identificadores únicos (CNPJs) por meio do algoritmo SHA-256, assegurando conformidade com requisitos de privacidade.
    \item \textbf{Matplotlib e Seaborn:} Para criação de gráficos e visualizações, auxiliando na análise exploratória e na identificação de padrões.
    \item \textbf{Scikit-learn:} Para implementação do algoritmo \textit{Isolation Forest}, aplicado na detecção de anomalias.
\end{itemize}

\subsubsection{Justificativa da Escolha}
A utilização da versão gratuita do \textit{Google Colab} foi motivada por sua acessibilidade, disponibilidade universal e suporte a projetos colaborativos. Essa versão permitiu o desenvolvimento completo do estudo sem custos adicionais, aproveitando os recursos computacionais adequados às demandas do projeto. A integração com bibliotecas robustas e bem estabelecidas no ecossistema Python assegurou a eficiência e a reprodutibilidade das análises realizadas.

\subsection{Compreensão dos Dados}

O \textit{dataset} original disponibilizado e utilizado na pesquisa possui \textbf{1.777.725 linhas e 19 colunas}. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{dicionariodedados.png}
    \caption{Dicionário de Dados Original}
    \label{fig:dicionario_dados}
\end{figure}


A Figura~\ref{fig:dicionario_dados} apresenta o dicionário de dados empregado no estudo, detalhando as variáveis analisadas, seus respectivos tipos de dados, uma breve descrição e exemplos de valores. Esse dicionário desempenhou um papel crucial na estruturação do modelo proposto, contribuindo para a organização e consistência na análise dos dados.

\subsection{Preparação dos Dados}

O primeiro passo na preparação dos dados foi a renomeação de todas as variáveis para o idioma inglês, com o objetivo de facilitar a replicação do estudo por outros pesquisadores e profissionais, ampliando sua acessibilidade e compreensão.

Na sequência, foram aplicadas às variáveis textuais as funções \texttt{str.strip()} e \texttt{str.lower()} em Python, para remover espaços em excesso e padronizar o formato em letras minúsculas. Essas transformações garantiram maior consistência nas informações, reduzindo erros decorrentes de formatos inconsistentes.

Para variáveis relacionadas a datas, foi utilizado \texttt{pd.to\_datetime} (biblioteca Pandas), que converteu os valores em um formato padronizado. O parâmetro \texttt{errors='coerce'} tratou valores inválidos, substituindo-os por nulos (NaT). Além disso, o método \texttt{.dt.normalize()} ajustou as datas ao horário padrão, eliminando informações desnecessárias sobre tempo. Essas mudanças asseguraram uniformidade e integridade dos dados temporais.

No tratamento de variáveis numéricas com valores ausentes, foi utilizado o comando:
\begin{itemize}
    \item \texttt{fillna(0.0).astype(int)}: Substituiu valores nulos por 0.0 e converteu as colunas para o tipo inteiro, garantindo consistência no formato.
\end{itemize}

Variáveis com alta concentração de valores nulos, como \texttt{name}, \texttt{cnae}, \texttt{status}, \texttt{address}, \texttt{city} e \texttt{neighborhood}, apresentaram 59 registros ausentes devido a limitações na fonte de dados (Receita Federal do Brasil). Como esses registros se referiam a CNPJs baixados sem informações adicionais, optou-se por excluí-los, preservando a integridade do \textit{dataset}.

\subsubsection{Criação de Novas Variáveis}

Para validar a hipótese de alterações cadastrais, foram criadas variáveis booleanas: \texttt{addressChanged}, \texttt{nameChanged}, \texttt{emailChanged} e \texttt{cnaeChanged}, indicando alterações nas respectivas informações ao longo do tempo. O processo envolveu:
\begin{enumerate}
    \item Comparação de valores (atual e anterior) para cada cliente;
    \item Identificação de alterações específicas dentro do mesmo \textit{customerID};
    \item Armazenamento dos resultados em colunas booleanas correspondentes.
\end{enumerate}

Além disso, a variável \texttt{customerID} foi anonimizada com o algoritmo \textbf{SHA-256}, e a coluna original foi excluída. A nova coluna \texttt{customer} foi posicionada como a primeira no \textit{dataset}.

\subsubsection{Transformações Relacionadas ao Tempo de Fundação}

Foi criada a variável \texttt{riskFoundation}, classificando empresas com base no número de anos desde sua fundação:
\begin{itemize}
    \item \textbf{High Risk:} Menos de 3 anos;
    \item \textbf{Low Risk:} Mais de 20 anos;
    \item Faixas intermediárias (\textit{Moderate-High, Moderate e Low-Moderate}) refletem níveis progressivos de estabilidade.
\end{itemize}

Para investigar inconsistências entre tempo de atividade e data de fundação, foi criada a variável \texttt{registrationAlert}, que sinaliza casos em que a data de registro diverge do período de atividade esperado.

\subsubsection{Análises de Crescimento Atípico}

Foram desenvolvidas variáveis auxiliares para identificar padrões atípicos:
\begin{enumerate}
    \item \textbf{Comportamento no Volume de Consultas:} A variável \texttt{queriesIncreaseAlert} foi criada ao calcular a média móvel de 3 meses para a variável \texttt{queries} e identificar aumentos superiores a 30\%.
    \item \textbf{Crescimento no Maior Acúmulo:} A variável \texttt{quantityGreaterAccumulationAlert} sinalizou aumentos significativos na variável \texttt{quantityGreaterAccumulation}, com base em uma média móvel de 3 meses e aumento superior a 30\%.
    \item \textbf{Crescimento no Débito Total:} A variável \texttt{quantityDebitAlert} identificou crescimentos atípicos no débito total, seguindo a mesma metodologia.
\end{enumerate}

Após essas análises, colunas auxiliares foram removidas, mantendo o \textit{dataset} enxuto e organizado.

\subsubsection{Normalização e Preparo Final}

Variáveis como \texttt{amountDebit} e \texttt{amountGreaterAccumulation} foram normalizadas com \texttt{np.log1p} para reduzir assimetrias e ajustadas em faixas categóricas (\textit{Very Low, Low, Medium, High}). A análise inicial mostrou distribuições equilibradas após a remoção de \textit{outliers} com base no \textit{Intervalo Interquartil} (IQR).

A variável \texttt{default} foi criada a partir da classificação do \textit{score} de crédito, indicando alto risco de inadimplência (\texttt{True}) para clientes com \textit{score} de crédito `d' ou `e'. Variáveis como \texttt{customer} e \texttt{updatedMonth} foram removidas por não contribuírem diretamente para a detecção de fraudes financeiras.

\subsubsection{\textit{Dataset} Final}
Após a preparação, o \textit{dataset} final foi composto pelas seguintes variáveis relevantes, otimizadas para aplicação no modelo \textit{Isolation Forest}, garantindo eficiência na detecção de fraudes financeiras.


Essas etapas resultaram em um \textit{dataset} robusto, focado em variáveis relevantes para a aplicação do modelo \textit{Isolation Forest}, garantindo maior eficiência na detecção de fraudes financeiras.


\subsection{Modelagem}
\subsubsection{Modelagem com o CBA}
O estudo iniciou com a aplica\c{c}\~{a}o do modelo CBA (Classification Based on Associations), um m\'etodo de aprendizado supervisionado que combina classifica\c{c}\~{a}o e minera\c{c}\~{a}o de regras de associa\c{c}\~{a}o. O CBA foi escolhido pela capacidade de gerar regras interpret\'aveis, facilitando a compreens\~{a}o dos padr\~{o}es associados \`a fraude financeira.

\begin{enumerate}
    \item \textbf{Constru\c{c}\~{a}o da Vari\'avel Alvo}
    
    A vari\'avel \textit{default} foi criada a partir da coluna \textit{rating}, em que registros com valores \textit{'d'} ou \textit{'e'} foram classificados como \textbf{True} (risco de fraude), e os demais como \textbf{False}. A coluna original foi exclu\'ida para simplifica\c{c}\~{a}o do conjunto de dados.

    \item \textbf{Resultados do Modelo}
    
    O modelo gerou um conjunto de regras de associa\c{c}\~{a}o com confian\c{c}a e \textit{lift} iguais a 1.0, identificando padr\~{o}es \textit{\'obvios}, como:
    \begin{itemize}
        \item \textbf{statusActive\_True} (suporte: 81,51\%)
        \item \textbf{nameChanged\_False} (suporte: 99,39\%)
        \item \textbf{emailChanged\_False} (suporte: 92,43\%)
    \end{itemize}

    Apesar das correla\c{c}\~{o}es identificadas, o modelo n\~{a}o foi capaz de capturar a complexidade real das fraudes, sendo prejudicado pelo desbalanceamento severo das classes (93,18\% \textbf{False} e 6,82\% \textbf{True}).
\end{enumerate}


\subsubsection{Modelagem com o Isolation Forest}

Devido \`as limita\c{c}\~{o}es da aplicabilidade do CBA ao dataset, foi adotado o paradigma de detec\c{c}\~{a}o de anomalias utilizando o Isolation Forest, da biblioteca Scikit-learn. O Isolation Forest foi escolhido pela sua capacidade de identificar padr\~{o}es at\'ipicos em grandes volumes de dados, sendo insens\'ivel ao desbalanceamento das classes.

\begin{enumerate}
    \item \textbf{Prepara\c{c}\~{a}o dos Dados}
    \begin{itemize}
        \item \textbf{Normaliza\c{c}\~{a}o}: Padroniza\c{c}\~{a}o das vari\'aveis para manter consist\^encia de escala.
        \item \textbf{Remo\c{c}\~{a}o de Outliers}: Exclus\~{a}o de valores extremos que poderiam distorcer o modelo.
        \item \textbf{Transforma\c{c}\~{a}o de Vari\'aveis}: Aplica\c{c}\~{a}o do One-Hot Encoding para converter vari\'aveis categ\'oricas em formato num\'erico. Vari\'aveis transformadas inclu\'iram altera\c{c}\~{o}es cadastrais, alertas, estado do cliente e \textit{riskFoundation} (codificada em \textit{High}, \textit{Moderate} e \textit{Low}).
    \end{itemize}

    \item \textbf{Configura\c{c}\~{a}o Inicial do Modelo}
    \begin{itemize}
        \item \textbf{Taxa de Contamina\c{c}\~{a}o}: 5\% (\texttt{contamination=0.05}).
        \item \textbf{Random State}: 42, garantindo reprodutibilidade.
    \end{itemize}

    Ap\'os o primeiro processamento, os resultados foram avaliados utilizando as m\'etricas \textbf{Precis\~{a}o}, \textbf{Recall} e \textbf{F1-Score}:
    \begin{itemize}
        \item \textbf{Precis\~{a}o}: 0.0225
        \item \textbf{Recall}: 0.2238
        \item \textbf{F1-Score}: 0.0408
    \end{itemize}

    \item \textbf{Ajuste de Par\^ametros com Cross-Validation}
    Para aprimorar o desempenho do modelo Isolation Forest, foi aplicada a t\'ecnica de Cross-Validation, buscando os melhores par\^ametros para otimiza\c{c}\~{a}o. Os resultados do processo foram:
    \begin{itemize}
        \item \textbf{Melhores Par\^ametros}:
        \begin{itemize}
            \item \texttt{contamination}: 0.01
            \item \texttt{max\_samples}: 0.6
            \item \texttt{n\_estimators}: 100
        \end{itemize}
        \item \textbf{Melhor F1-Score}: 0.0337
    \end{itemize}

    \textbf{Interpreta\c{c}\~{a}o:} Apesar do ajuste fino dos hiperpar\^ametros, o F1-Score final apresentou uma leve redu\c{c}\~{a}o em compara\c{c}\~{a}o ao processamento inicial (0.0408 para 0.0337). Esse comportamento pode ser atribuído \`a tentativa de ajustar a taxa de contamina\c{c}\~{a}o para reduzir falsos positivos, o que afetou o equil\'ibrio entre precis\~{a}o e recall.

    \item \textbf{An\'alise Comparativa}
    \begin{enumerate}
        \item \textbf{Modelo CBA}:
        \begin{itemize}
            \item \textbf{Desempenho}: Baixo, com identifica\c{c}\~{a}o de padr\~{o}es triviais e sensibilidade ao desbalanceamento.
        \end{itemize}
        \item \textbf{Isolation Forest}:
        \begin{itemize}
            \item \textbf{Desempenho Inicial}: Melhor capacidade de identificar fraudes, mas com baixa precis\~{a}o e F1-Score limitado devido ao grande n\'umero de falsos positivos.
            \item \textbf{Desempenho Ajustado}: Ap\'os a aplica\c{c}\~{a}o da cross-validation, os par\^ametros otimizados n\~{a}o proporcionaram um ganho significativo no F1-Score, indicando que a natureza do problema (fraudes esparsas e complexas) pode exigir abordagens adicionais.
        \end{itemize}
    \end{enumerate}

    \item \textbf{Propostas de Melhorias}
    \begin{enumerate}
        \item \textbf{Modelos H\'ibridos}: Integrar m\'etodos de detec\c{c}\~{a}o de anomalias (Isolation Forest) com modelos supervisionados, como Random Forest ou XGBoost, para combinar as vantagens dos dois paradigmas.
        \item \textbf{Redu\c{c}\~{a}o de Falsos Positivos}: Ajustar a taxa de contamina\c{c}\~{a}o de forma din\^amica ou aplicar p\'os-processamento para filtrar anomalias irrelevantes.
        \item \textbf{Valida\c{c}\~{a}o Especializada}: Submeter os registros classificados como an\^omalos \`a valida\c{c}\~{a}o com especialistas, a fim de confirmar sua rela\c{c}\~{a}o com fraudes reais.
        \item \textbf{Balanceamento dos Dados}: Aplicar t\'ecnicas como SMOTE no conjunto de dados inicial para treinar modelos supervisionados que lidem melhor com a classe minorit\'aria.
    \end{enumerate}
\end{enumerate}

A etapa de modelagem evidenciou as limita\c{c}\~{o}es do modelo CBA na detec\c{c}\~{a}o de fraudes em dados desbalanceados e a maior efici\^encia do Isolation Forest na identifica\c{c}\~{a}o de comportamentos at\'ipicos. Apesar do ajuste dos hiperpar\^ametros com cross-validation, o modelo apresentou um F1-Score de 0.0337, sugerindo a necessidade de abordagens complementares para aumentar o desempenho.

Os pr\'oximos passos incluem a valida\c{c}\~{a}o pr\'atica das anomalias detectadas, a implementa\c{c}\~{a}o de modelos h\'ibridos e o refinamento de t\'ecnicas para redu\c{c}\~{a}o de falsos positivos e melhoria do equil\'ibrio entre precis\~{a}o e recall.

\subsection{Avalia\c{c}\~{a}o}

Para realizar a avalia\c{c}\~{a}o do modelo, o Isolation Forest gera predi\c{c}\~{o}es que classificam as observa\c{c}\~{o}es como normais (1) ou anomalias (-1). A compara\c{c}\~{a}o dessas predi\c{c}\~{o}es com os r\'otulos verdadeiros (\textit{true\_labels}) permite calcular m\'etricas de desempenho, como precis\~{a}o, recall, acur\'acia e F1-Score. Contudo, como o dataset original n\~{a}o possu\'ia r\'otulos verdadeiros, optou-se por simular r\'otulos com base em regras de neg\'ocio e conhecimento do dom\'inio, considerando a natureza e o contexto dos dados analisados.

Existem v\'arias t\'ecnicas para lidar com a aus\^encia de r\'otulos, como a inje\c{c}\~{a}o de anomalias sint\'eticas ou a avalia\c{c}\~{a}o qualitativa das observa\c{c}\~{o}es classificadas pelo modelo. Neste estudo, a simula\c{c}\~{a}o baseada em regras foi escolhida devido \`a familiaridade com o contexto do dataset e \`a identifica\c{c}\~{a}o de padr\~{o}es consistentes em golpes financeiros observados na CISP.

Com base em regras de neg\'ocio, foi criada a vari\'avel \textit{true\_labels} no dataset, fundamentada em caracter\'isticas recorrentes identificadas em casos reais de fraudes financeiras. As principais caracter\'isticas observadas foram:
\begin{enumerate}
    \item \textbf{Crescimento At\'ipico no Volume de Compras}: Representado pelas vari\'aveis \textit{quantityGreaterAccumulationAlert} e \textit{quantityDebitAlert}, que assumem o valor \textbf{True} em situa\c{c}\~{o}es de comportamento an\^omalo, como um aumento significativo e incomum no volume de compras.
    \item \textbf{Aumento no Comportamento de Consultas}: Identificado pela vari\'avel \textit{queriesIncreaseAlert}, que sinaliza \textbf{True} quando h\'a um aumento expressivo no n\'umero de consultas realizadas, comportamento frequentemente associado a tentativas de golpe.
    \item \textbf{Inconsist\^encia no Tempo de Atividade vs. Data de Funda\c{c}\~{a}o}: Alguns CNPJs permaneceram inativos por longos per\'iodos e foram posteriormente reativados sem hist\'orico de relacionamento com ind\'ustrias, um comportamento t\'ipico em golpes financeiros. Essa caracter\'istica \'e representada pela vari\'avel \textit{riskFoundation}, com o valor \textit{high}.
\end{enumerate}

Essas regras foram aplicadas como crit\'erios para identificar anomalias no dataset, resultando na cria\c{c}\~{a}o de uma base de r\'otulos (\textit{true\_labels}) consistente e alinhada ao conhecimento de dom\'inio para a detec\c{c}\~{a}o de fraudes financeiras.

Com a aplica\c{c}\~{a}o da t\'ecnica, o modelo Isolation Forest identificou 1.338 casos de anomalias no dataset, indicando observa\c{c}\~{o}es com padr\~{o}es at\'ipicos e potencialmente fraudulentos.

\subsection{Implanta\c{c}\~{a}o}

O modelo ser\'a implementado atrav\'es da cria\c{c}\~{a}o de um servi\c{c}o (API), que recebe os dados das solicita\c{c}\~{o}es e retorna uma classifica\c{c}\~{a}o em tempo real. Esse modelo ser\'a integrado ao sistema de motor de cr\'edito atualmente disponibilizado pela CISP aos seus associados. Vale destacar que o fluxo de concess\~{a}o de cr\'edito descrito j\'a est\'a em opera\c{c}\~{a}o com os associados, e a inclus\~{a}o da consulta \`a API de detec\c{c}\~{a}o de golpes financeiros ser\'a a principal novidade incorporada ao processo.

A Figura abaixo ilustra o fluxo futuro de concess\~{a}o de cr\'edito para os associados da CISP que utilizam o motor de cr\'edito. Esse fluxo operacional segue as etapas descritas a seguir:

\begin{figure}[H]
    \centering
    \includegraphics[angle=90, width=0.4\textheight]{diagrama.png}
    \caption{Exemplo de imagem rotacionada ocupando grande parte da p\'agina.}
    \label{fig:imagem_rotacionada}
\end{figure}



\begin{enumerate}
    \item \textbf{Identifica\c{c}\~{a}o do Cliente}
    Inicialmente, o cliente do associado \'{e} identificado como sendo um novo cliente (sem hist\'orico de movimenta\c{c}\~{o}es com o associado) ou um cliente ativo (com hist\'orico de transa\c{c}\~{o}es anteriores). O cliente, ent\~{a}o, realiza uma solicita\c{c}\~{a}o de compra junto ao associado.

    \item \textbf{Consulta Inicial ao ERP}
    O sistema CRM do associado envia a solicita\c{c}\~{a}o ao ERP para verificar se o cliente possui cr\'edito dispon\'ivel.
    \begin{itemize}
        \item Caso tenha cr\'edito suficiente e em vig\^{e}ncia: O pedido de compra \'{e} aprovado automaticamente, e o cliente \'{e} comunicado.
        \item Caso n\~{a}o tenha cr\'edito: Se o cr\'edito estiver expirado, comprometido ou insuficiente, o ERP encaminha a solicita\c{c}\~{a}o ao motor de cr\'edito da CISP para realiza\c{c}\~{a}o de uma an\'alise detalhada.
    \end{itemize}

    \item \textbf{An\'alise pelo Motor de Cr\'edito da CISP}
    O motor de cr\'edito da CISP cont\'em todas as regras personalizadas para executar a an\'alise de cr\'edito, de acordo com a pol\'itica de cr\'edito da empresa associada. O fluxo de an\'alise segue as etapas abaixo:
    \begin{itemize}
        \item \textbf{Consulta \`as Fontes de Informa\c{c}\~{a}o}: O motor consulta diversas bases de dados, incluindo:
        \begin{itemize}
            \item Dados cadastrais em fontes p\'ublicas.
            \item Outras informa\c{c}\~{o}es definidas previamente pela empresa.
        \end{itemize}
        Como parte das boas pr\'aticas de gest\~{a}o de risco de cr\'edito, as configura\c{c}\~{o}es dessas fontes e pol\'iticas devem ser alinhadas \`as diretrizes da pol\'itica de cr\'edito da empresa.
        \item \textbf{C\'alculo do Risco de Performance}: Ap\'os a coleta das informa\c{c}\~{o}es, o motor realiza o c\'alculo do risco de performance do cliente. Esse c\'alculo considera aspectos como hist\'orico de cr\'edito e a capacidade de pagamento do cliente.
        \item \textbf{Inclus\~{a}o da Consulta \`a API de Detec\c{c}\~{a}o de Golpes}: Como novidade no processo, o motor de cr\'edito agora consulta a API de detec\c{c}\~{a}o de golpes financeiros, que retorna um resultado indicando se a transa\c{c}\~{a}o \'{e} caracterizada como um poss\'ivel golpe financeiro ou n\~{a}o.
    \end{itemize}

    \item \textbf{Tomada de Decis\~{a}o}
    Ap\'os a conclus\~{a}o da an\'alise de risco e a verifica\c{c}\~{a}o contra golpes financeiros, o motor de cr\'edito aplica as regras de tomada de decis\~{a}o, conforme configurado na pol\'itica de cr\'edito da empresa associada.
    \begin{itemize}
        \item \textbf{Exemplo}: Se o cliente possui uma classifica\c{c}\~{a}o de rating "boa" e uma alta probabilidade de golpe financeiro, o sistema analisa os crit\'erios de recusa contidos na pol\'itica de cr\'edito do associado e toma uma decis\~{a}o automatizada.
    \end{itemize}

    \item \textbf{Gera\c{c}\~{a}o do Dossi\^e}
    Toda decis\~{a}o \'{e} acompanhada por um dossi\^e, que re\'une diversas informa\c{c}\~{o}es e relat\'orios para garantir transpar\^{e}ncia e rastreabilidade do processo. Esse dossi\^e inclui dados coletados, resultados das an\'alises e as justificativas para a decis\~{a}o final.
\end{enumerate}

O fluxo de concess\~{a}o de cr\'edito j\'a existente nos associados da CISP ser\'a aprimorado com a inclus\~{a}o da API de detec\c{c}\~{a}o de golpes financeiros, adicionando uma camada de prote\c{c}\~{a}o contra fraudes. Essa integra\c{c}\~{a}o n\~{a}o apenas refor\c{c}ar\'a a gest\~{a}o de riscos, mas tamb\'em contribuir\'a para maior agilidade e precis\~{a}o nas decis\~{o}es de cr\'edito automatizadas.



\section{CONCLUSÃO}
\subsection{Objetivos do Estudo}

O objetivo do estudo foi validar um modelo de intelig\^{e}ncia artificial para identificar golpes financeiros na concess\~{a}o de cr\'edito, que n\~{a}o est\~{a}o presentes no modelo de score de cr\'edito atualmente desenvolvido pela empresa, e implementar esse modelo no motor de cr\'edito. 

A implementa\c{c}\~{a}o do algoritmo CBA (Classification Based Associations), que \'{e} um algoritmo supervisionado e atua no paradigma de classifica\c{c}\~{a}o, n\~{a}o obteve resultados satisfat\'orios devido \`as caracter\'{\i}sticas do dataset, que \'{e} desbalanceado, ou seja, possui poucas classes que possam identificar golpes financeiros. Existem t\'ecnicas diversas que podem ser implementadas para tentar balancear o dataset e encontrar poss\'{\i}veis sinais de golpes financeiros. 

Contudo, como conclus\~{a}o, o paradigma de classifica\c{c}\~{a}o n\~{a}o se aplica ao objetivo do estudo, que \'{e} a identifica\c{c}\~{a}o de golpes financeiros. O modelo CBA trouxe, atrav\'es da sua aplica\c{c}\~{a}o, uma vertente interessante para identificar quais regras (antecessores e predecessores) podem ser \'{u}teis para compreender quais fatores realmente influenciam diretamente os golpes financeiros. Vale ressaltar que o modelo CBA \'{e} supervisionado, ou seja, depende da exist\^{e}ncia de uma vari\'{a}vel target, que, no estudo, foi criada com base na utiliza\c{c}\~{a}o da vari\'{a}vel score de cr\'edito. Os valores \textit{D} e \textit{E}, que identificam inadimpl\^{e}ncia alta, foram selecionados, mas n\~{a}o caracterizam necessariamente um golpe financeiro, o que pode ser uma limita\c{c}\~{a}o do dataset e influenciar no resultado final do estudo.

O estudo evoluiu para a utiliza\c{c}\~{a}o de outra t\'ecnica ou paradigma: a detec\c{c}\~{a}o de anomalias. Esse paradigma foi escolhido devido ao fato de o dataset apresentar poucos registros discrepantes. A aplica\c{c}\~{a}o do modelo Isolation Forest, que \'{e} n\~{a}o supervisionado e n\~{a}o depende da cria\c{c}\~{a}o de uma vari\'{a}vel target, mostrou-se mais promissora, mesmo possuindo m\'etricas de avalia\c{c}\~{a}o a serem melhoradas. 

Ap\'os a busca por otimizar os par\^ametros do algoritmo, o pr\'{o}ximo passo ser\'{a} a aplica\c{c}\~{a}o de t\'ecnicas para melhorar as m\'etricas de avalia\c{c}\~{a}o, incluindo a sele\c{c}\~{a}o de vari\'{a}veis. Com isso, espera-se que o modelo Isolation Forest possa ser avaliado e apresentar m\'etricas aceit\'aveis para utiliza\c{c}\~{a}o.

\subsection{Contribui\c{c}\~{o}es T\'ecnicas}
\subsubsection{Avalia\c{c}\~{a}o Cr\'itica do Paradigma de Classifica\c{c}\~{a}o com o CBA}
- O estudo implementou o algoritmo \textbf{CBA (Classification Based Associations)}, que utiliza um paradigma supervisionado para identificar golpes financeiros na concess\~{a}o de cr\'edito. Essa abordagem, baseada em classifica\c{c}\~{a}o, mostrou-se inadequada devido \`as caracter\'{\i}sticas espec\'{\i}ficas do dataset, como o desbalanceamento severo entre classes e a depend\^{e}ncia de uma vari\'{a}vel target para treinar o modelo.
- Apesar disso, a aplica\c{c}\~{a}o do CBA permitiu identificar regras associativas (antecessores e predecessores) que influenciam diretamente os golpes financeiros. Essas regras oferecem insights valiosos para a constru\c{c}\~{a}o de modelos futuros e compreens\~{a}o dos fatores subjacentes aos golpes.

\subsubsection{Transi\c{c}\~{a}o para Detec\c{c}\~{a}o de Anomalias com Isolation Forest}
- Ap\'os a an\'alise das limita\c{c}\~{o}es do paradigma de classifica\c{c}\~{a}o, o estudo avan\c{c}ou para o paradigma de detec\c{c}\~{a}o de anomalias. A implementa\c{c}\~{a}o do modelo \textbf{Isolation Forest}, que \'{e} n\~{a}o supervisionado, eliminou a necessidade de cria\c{c}\~{a}o de uma vari\'{a}vel target, sendo mais apropriado para lidar com datasets desbalanceados e esparsos.
- O modelo demonstrou ser promissor, mesmo com m\'etricas de avalia\c{c}\~{a}o iniciais ainda pass\'{\i}veis de melhorias. Essa transi\c{c}\~{a}o representou uma adapta\c{c}\~{a}o significativa para atender aos desafios apresentados pelo dataset.

\subsubsection{Identifica\c{c}\~{a}o e Abordagem de Limita\c{c}\~{o}es do Dataset}
- O estudo reconheceu as limita\c{c}\~{o}es do dataset utilizado, como a aus\^{e}ncia de registros rotulados como golpes financeiros e a predomin\^ancia de classes majorit\'arias. Essas caracter\'{\i}sticas dificultam a aplica\c{c}\~{a}o de t\'ecnicas tradicionais de classifica\c{c}\~{a}o.
- Foram exploradas possibilidades de balanceamento e otimiza\c{c}\~{a}o do dataset, com foco na cria\c{c}\~{a}o de vari\'{a}veis mais representativas e robustas para capturar sinais de anomalias e fraudes.

\subsubsection{Planejamento de Melhorias Futuras}
- O estudo prop\^os estrat\'egias claras para aprimorar o desempenho do modelo Isolation Forest, como a sele\c{c}\~{a}o de vari\'{a}veis mais relevantes e a aplica\c{c}\~{a}o de t\'ecnicas de otimiza\c{c}\~{a}o de par\^ametros.
- Foi destacado o pr\'oximo passo de avalia\c{c}\~{a}o do modelo com m\'etricas mais robustas e ajustadas para uso pr\'atico no motor de cr\'edito da empresa.

\subsection{Contribui\c{c}\~{o}es para a Comunidade Cient\'ifica}

\subsubsection{Inova\c{c}\~{a}o na Detec\c{c}\~{a}o de Fraudes em Modelos de Cr\'edito}
- A an\'alise revelou que, embora os paradigmas de classifica\c{c}\~{a}o sejam amplamente utilizados, sua aplica\c{c}\~{a}o pode ser inadequada em contextos de fraudes financeiras devido ao desbalanceamento dos dados. Essa conclus\~{a}o contribui para a comunidade cient\'{\i}fica ao demonstrar os limites do uso de algoritmos supervisionados, como o CBA, em problemas com dados esparsos.

\subsubsection{Valida\c{c}\~{a}o de Abordagens N\~{a}o Supervisionadas em Dados Desbalanceados}
- A ado\c{c}\~{a}o do \textbf{Isolation Forest} como paradigma alternativo para detec\c{c}\~{a}o de golpes financeiros oferece uma abordagem inovadora que n\~{a}o depende de vari\'{a}veis target. Essa t\'ecnica refor\c{c}a a viabilidade de m\'etodos n\~{a}o supervisionados para problemas de fraude e pode inspirar novos estudos que explorem cen\'arios similares.

\subsubsection{Base de Conhecimento sobre Regras Associativas em Fraudes}
- A aplica\c{c}\~{a}o do CBA, embora limitada para o objetivo final, gerou um conjunto de regras interpret\'aveis que podem ser aproveitadas em futuros estudos e implementa\c{c}\~{o}es. Isso amplia o entendimento de padr\~{o}es e comportamentos t\'ipicos de golpes financeiros, sendo \'{u}til tanto para pesquisadores quanto para desenvolvedores de sistemas de cr\'edito.

\subsubsection{Demonstra\c{c}\~{a}o Pr\'atica da Import\^ancia de Prepara\c{c}\~{a}o de Dados}
- O estudo destacou a complexidade e a import\^ancia de preparar um dataset adequado para an\'alise de fraudes. A comunidade cient\'{\i}fica se beneficia dessa experi\^{e}ncia ao reconhecer que a etapa de engenharia de dados \'{e} cr\'{\i}tica para o sucesso de modelos preditivos.

\subsubsection{Direcionamento para Estudos Futuros}
- O trabalho prop\^os dire\c{c}\~{o}es claras para pesquisas futuras, como:
  \begin{itemize}
      \item Avalia\c{c}\~{a}o de t\'ecnicas h\'{\i}bridas que combinem detec\c{c}\~{a}o de anomalias e m\'etodos supervisionados.
      \item Investiga\c{c}\~{a}o de novas vari\'{a}veis derivadas que possam capturar sinais de golpes financeiros.
      \item Estudo sobre t\'ecnicas de balanceamento de dados em cen\'arios de fraudes.
  \end{itemize}

O estudo contribuiu tecnicamente ao explorar e avaliar diferentes paradigmas para identifica\c{c}\~{a}o de golpes financeiros, desde a implementa\c{c}\~{a}o do CBA at\'e a ado\c{c}\~{a}o do Isolation Forest. Para a comunidade cient\'{\i}fica, os insights gerados sobre os limites de algoritmos supervisionados em datasets desbalanceados, aliados \`as propostas para futuras pesquisas, enriquecem o campo de detec\c{c}\~{a}o de fraudes e modelagem de cr\'edito. Essas contribui\c{c}\~{o}es oferecem n\~{a}o apenas um avan\c{c}o pr\'atico para a empresa, mas tamb\'{\e}m uma base s\'olida para estudos acad\^{e}micos e industriais futuros.

\subsection{Trabalhos Futuros}

Os trabalhos futuros concentrar-se-\~{a}o no desenvolvimento de um novo algoritmo de score de cr\'edito que incorpore caracter\'isticas ainda n\~{a}o utilizadas, como a detec\c{c}\~{a}o de anomalias. Esse novo modelo ser\'a baseado em vari\'aveis previamente calculadas e regras aplicadas diretamente, eliminando a necessidade de criar um preditor espec\'ifico para identificar golpes financeiros. Dessa forma, as vari\'aveis incluir\~{a}o an\'alises robustas que resultem em um score de cr\'edito mais confi\'avel e resiliente.

Al\'em disso, futuras pesquisas incluir\~{a}o uma revis\~{a}o bibliogr\'afica aprofundada sobre o desenvolvimento de modelos de score de cr\'edito. Essa revis\~{a}o abordar\'a as vari\'aveis mais utilizadas e as t\'ecnicas de intelig\^encia artificial recomendadas. Complementarmente, ser\~{a}o realizados experimentos pr\'aticos com a implementa\c{c}\~{a}o de diferentes m\'etodos e modelos, buscando identificar as abordagens mais eficazes e adequadas para o contexto do estudo.

Outra linha de pesquisa envolver\'a a implementa\c{c}\~{a}o de uma lista dos principais modelos de classifica\c{c}\~{a}o, detec\c{c}\~{a}o de anomalias e aprendizado profundo, utilizando redes neurais. A aplica\c{c}\~{a}o de redes neurais poder\'a trazer resultados satisfat\'orios devido \`a sua capacidade de adapta\c{c}\~{a}o em cen\'arios complexos. A avalia\c{c}\~{a}o comparativa entre os diferentes algoritmos implementados permitir\'a identificar a solu\c{c}\~{a}o mais adequada.

Por fim, o aprofundamento em t\'ecnicas de data mining ser\'a uma \área de estudo relevante. Como demonstrado no trabalho atual, foi necess\'ario um esfor\c{c}o significativo para preparar o dataset antes da aplica\c{c}\~{a}o dos modelos. O uso de data mining permitir\'a extrair conhecimento valioso dos dados e otimizar o processo de prepara\c{c}\~{a}o para an\'alises futuras.


% REFERÊNCIAS
\newpage
\bibliographystyle{abnt-alf}
\bibliography{biblproj} % Certifique-se de que o arquivo biblproj.bib está correto










\end{document}
